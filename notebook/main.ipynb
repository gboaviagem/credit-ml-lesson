{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit card fraud detection\n",
    "\n",
    "Modelagem para previsão de fraude em cartões de crédito, usando um **dataset modificado** a partir [desta base original do Kaggle](https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud).\n",
    "\n",
    "As colunas são todas codificadas, exceto por estas:\n",
    "\n",
    "- Time: Number of seconds elapsed between this transaction and the first transaction in the dataset\n",
    "- Amount: The feature 'Amount' is the transaction Amount, this feature can be used for example-dependant cost-sensitive learning\n",
    "\n",
    "SOBRE O DATASET ORIGINAL (extraído do Kaggle):\n",
    "```\n",
    "It is important that credit card companies are able to recognize fraudulent credit card transactions so that customers are not charged for items that they did not purchase.\n",
    "\n",
    "The dataset contains transactions made by credit cards in September 2013 by European cardholders.\n",
    "This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.\n",
    "\n",
    "It contains only numerical input variables which are the result of a PCA transformation. Unfortunately, due to confidentiality issues, we cannot provide the original features and more background information about the data. Features V1, V2, … V28 are the principal components obtained with PCA, the only features which have not been transformed with PCA are 'Time' and 'Amount'. Feature 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature 'Amount' is the transaction Amount, this feature can be used for example-dependant cost-sensitive learning. Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise.\n",
    "\n",
    "Given the class imbalance ratio, we recommend measuring the accuracy using the Area Under the Precision-Recall Curve (AUPRC). Confusion matrix accuracy is not meaningful for unbalanced classification.\n",
    "```\n",
    "Benchmarking: AuC in [0.85, 0.95]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"../data/creditcard.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Class'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA\n",
    "\n",
    "A EDA deve ser feita com cautela. Iremos passar por ela mais rapidamente, pelo tempo da aula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sweetviz as sv\n",
    "\n",
    "# Prepare-se: EDA automática tende a demorar!\n",
    "# my_report = sv.analyze(df, target_feat='Class')\n",
    "# my_report.show_html() # Default arguments will generate to \"SWEETVIZ_REPORT.html\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline\n",
    "\n",
    "Vamos criar um baseline de negócio, algum critério de negócio que o cliente já consegue atingir hoje. Em geral, espera-se que a discussão comercial já consiga extrair do próprio cliente esse baseline e critério de sucesso da modelagem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "fig = px.histogram(\n",
    "    df, x=\"Amount\", nbins=50, log_y=True, title=\"Log Histogram\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "\n",
    "kbins = KBinsDiscretizer(n_bins=10, encode='ordinal', strategy='quantile')\n",
    "df['AMOUNT_QUANTILE'] = kbins.fit_transform(df[['Amount']]).flatten()\n",
    "df.groupby('AMOUNT_QUANTILE').agg({'Class': ['mean']}).plot(kind='bar', figsize=(15, 5), title='Class by Amount Quantile')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmarking: filtrar a população usando o decil D9 de Amount\n",
    "df[df['AMOUNT_QUANTILE'].isin([9.0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_high = df[df['AMOUNT_QUANTILE'].isin([9.0])].copy()\n",
    "print(f\"Taxa de fraude média da base: {df['Class'].mean()}\")\n",
    "print(f\"Taxa de fraude no grupo de bench: {df_high['Class'].mean()}\")\n",
    "\n",
    "print(\"Lift:\", df_high['Class'].mean() / df['Class'].mean())\n",
    "print(\"Suporte:\", len(df_high) / len(df))\n",
    "print(\"Valor total das transações fraudadas na base de bench:\", df_high[df_high['Class'] == 1]['Amount'].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pré-processamento de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "y = df['Class'].copy()\n",
    "X = df.drop(columns=['Class', 'Time']).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputers\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "num_cols = X.select_dtypes(include=np.number).columns.values\n",
    "cat_cols = X.select_dtypes(exclude=np.number).columns.values\n",
    "\n",
    "num_imp = SimpleImputer(strategy='mean')\n",
    "X[num_cols] = num_imp.fit_transform(X[num_cols])\n",
    "\n",
    "cat_imp = SimpleImputer(strategy='most_frequent')\n",
    "X[cat_cols] = cat_imp.fit_transform(X[cat_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outliers\n",
    "# X[num_cols].describe()\n",
    "Q75 = X[num_cols].quantile(0.75)\n",
    "Q25 = X[num_cols].quantile(0.25)\n",
    "IQR = Q75 - Q25\n",
    "lower_lim = Q25 - 1.5 * IQR\n",
    "upper_lim = Q75 + 1.5 * IQR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upper_lim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[num_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Há categorias com cardinalidade muito baixa (outliers)?\n",
    "for col in cat_cols:\n",
    "    print(f\"{col}: {100 * np.round(X[col].value_counts().min() / len(X), 4)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Onde a célula tiver valor acima que upper_lim, nós daremos um \"replace\" por upper_lim\n",
    "X[num_cols] = np.where(X[num_cols] > upper_lim, upper_lim, X[num_cols])\n",
    "# Mesma coisa para o lower_lim\n",
    "X[num_cols] = np.where(X[num_cols] < lower_lim, lower_lim, X[num_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding\n",
    "# Cuidado ao usar o TargetEncoder agora, sem separar base de validação!\n",
    "from category_encoders.binary import BinaryEncoder\n",
    "\n",
    "enc = BinaryEncoder(handle_unknown='OTHERS')\n",
    "X_cat = enc.fit_transform(X[cat_cols])\n",
    "X_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.drop(columns=cat_cols, inplace=True)\n",
    "X = pd.concat([X, X_cat], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelagem v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_tr, X_ts, y_tr, y_ts = train_test_split(\n",
    "    X, y, test_size=0.3,\n",
    "    random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_tr.mean())\n",
    "print(y_ts.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=50)\n",
    "clf.fit(X_tr, y_tr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E agora?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Avaliação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us compute the roc_auc:\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "y_pred = clf.predict_proba(X_ts)[:, 1]\n",
    "roc_auc = roc_auc_score(y_ts, y_pred)\n",
    "roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "cm = confusion_matrix(y_pred=clf.predict(X_ts), y_true=y_ts)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0, 1])\n",
    "disp.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Confusion Matrix não precisa (não deve?) ser construída com o threshold de 0.5 no score! Mas qual threshold usar?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avaliação de negócio contra o baseline!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "\n",
    "X_baseline = X_ts[X_ts['AMOUNT_QUANTILE'] == 9.0].copy()\n",
    "X_baseline['SCORE'] = clf.predict_proba(X_baseline)[:, 1]\n",
    "X_baseline['Class'] = y_ts\n",
    "\n",
    "kbins2 = KBinsDiscretizer(n_bins=10, encode='ordinal', strategy='uniform')\n",
    "X_baseline['SCORE_RANGE'] = kbins2.fit_transform(X_baseline[['SCORE']]).flatten()\n",
    "X_baseline.groupby('SCORE_RANGE').agg({'Class': ['mean']}).plot(kind='bar', figsize=(15, 5), title='Class by Score Range')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_high = X_baseline.sort_values(by='SCORE', ascending=False).iloc[:int(0.1 * len(X_baseline)), :]\n",
    "\n",
    "print(f\"Taxa de fraude média da base: {y_ts.mean()}\")\n",
    "print(f\"Taxa de fraude na base escorada: {X_high['Class'].mean()}\")\n",
    "\n",
    "print(\"Lift:\", X_high['Class'].mean() / y_ts.mean())\n",
    "print(\"Suporte:\", len(X_high) / len(y_ts))\n",
    "print(\"Valor total das transações fraudadas na base escorada:\", X_high[X_high['Class'] == 1]['Amount'].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare com nosso baseline:\n",
    "```\n",
    "Taxa de fraude média da base: 0.001727485630620034\n",
    "Taxa de fraude no grupo de bench: 0.0029842362110732716\n",
    "Lift: 1.727502769445417\n",
    "Suporte: 0.10000807564420819\n",
    "Valor total das transações fraudadas na base de bench: 46965.89000000001\n",
    "```\n",
    "Deu certo?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelagem v2\n",
    "\n",
    "- Validação\n",
    "- Otimização hiperparamétrica\n",
    "- Uso de Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_tr, X_ts, y_tr, y_ts = train_test_split(\n",
    "    X, y, test_size=0.3,\n",
    "    random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "import pandas as pd\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/cross_validation.html\n",
    "def custom_cv_kfolds(X: pd.DataFrame, y: pd.Series, n_splits: int = 2):\n",
    "    \"\"\"Função para split customizado.\n",
    "\n",
    "    Esta função que um Generator que retorna os índices de treino e validação,\n",
    "    de acordo com o número de splits definido. Usamos um split de\n",
    "    KFolds, exceto que a classe minoritária da base de treino e sempre incluída\n",
    "    em sua totalidade.\n",
    "    \"\"\"\n",
    "    idx_fraud = np.where(y.to_numpy() == 1.0)[0]\n",
    "    idx_non_fraud = np.where(y.to_numpy() == 0.0)[0]\n",
    "    kf = KFold(n_splits=n_splits)\n",
    "    for _, (train_index, test_index) in enumerate(kf.split(X.iloc[idx_non_fraud, :])):\n",
    "        idx_tr = np.hstack([\n",
    "            idx_non_fraud[train_index],\n",
    "            idx_fraud\n",
    "        ])\n",
    "        idx_val = np.hstack([\n",
    "            idx_non_fraud[test_index],\n",
    "            idx_fraud\n",
    "        ])\n",
    "        yield idx_tr, idx_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=50)\n",
    "custom_cv = custom_cv_kfolds(X_tr, y_tr, n_splits=5)\n",
    "# Opções de scoring: https://scikit-learn.org/stable/modules/model_evaluation.html\n",
    "res = cross_validate(\n",
    "    clf, X_tr, y_tr, return_estimator=True,\n",
    "    cv=custom_cv, scoring='roc_auc', verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_ = res['estimator'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us compute the roc_auc:\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "y_pred = clf_.predict_proba(X_ts)[:, 1]\n",
    "roc_auc = roc_auc_score(y_ts, y_pred)\n",
    "roc_auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos tentar o uso de uma lib de otimização de hiperparâmetros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "\n",
    "opt = BayesSearchCV(\n",
    "    RandomForestClassifier(),\n",
    "    {\n",
    "        'max_depth': Integer(1, 9),\n",
    "        'max_features': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "        'bootstrap': [True, False],\n",
    "        'n_estimators': [5, 10, 30, 50, 80, 100, 200, 300, 500]\n",
    "    },\n",
    "    n_iter=3,\n",
    "    cv=3\n",
    ")\n",
    "\n",
    "opt.fit(X_tr, y_tr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usando a classe Pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from category_encoders.binary import BinaryEncoder\n",
    "\n",
    "numeric_pipe = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_pipe = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('encoder', BinaryEncoder(handle_unknown='OTHERS'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('numeric', numeric_pipe, num_cols),\n",
    "    ('categorical', categorical_pipe, cat_cols)\n",
    "])\n",
    "\n",
    "opt = BayesSearchCV(\n",
    "    RandomForestClassifier(),\n",
    "    {\n",
    "        'max_depth': Integer(1, 9),\n",
    "        'max_features': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "        'bootstrap': [True, False],\n",
    "        'n_estimators': [5, 10, 30, 50, 80, 100, 200, 300, 500]\n",
    "    },\n",
    "    n_iter=15,\n",
    "    cv=3\n",
    ")\n",
    "\n",
    "model = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('clf', opt)\n",
    "])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "credit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
